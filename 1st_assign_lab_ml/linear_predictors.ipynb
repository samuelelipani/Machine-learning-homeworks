{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalla libreria sklearn è possibile anche importare un comando che suddivide il dominio in training set e test set seguendo una precisa proporzione fornita come argomento: <br>\n",
    "<code> \n",
    "from sklearn.model_selection import train_test_split <br>\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "</code>"
   ]
  },
  {
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sl\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Position', 'Height cm', 'kg', 'Age']\n260\nThere are at least ten -1 and ten 1 in Y_training\nThere are no conflicts between training and test set\n78\nShape of training set: (182, 3)\nShape of test set: (78, 3)\nShape of training labels: (182,)\nShape of test labels: (78,)\ndimension of X in homogeneus coordinates is (182, 4)\n"
     ]
    }
   ],
   "source": [
    "IDnumber = 2025168\n",
    "#YOUR_ID , try also to change the seed to see the impact of random initialization on the results\n",
    "np.random.seed(IDnumber)\n",
    "\n",
    "#load the dataset\n",
    "filename = 'data/NBA.csv'\n",
    "NBA = csv.reader(open(filename, newline=''), delimiter=',')\n",
    "\n",
    "header = next(NBA) #skip first line\n",
    "print(header)\n",
    "\n",
    "dataset = np.array(list(NBA)).astype(int)\n",
    "\n",
    "X = dataset[:,1:4] #columns 1,2,3 contain the features\n",
    "Y = dataset[:,0]  # column 0: labels\n",
    "# first coloumn are already the labels, 1 if it's a center, 0 if point guard. We can shift the labels to get {-1,1} doubling the label and subtracting one  \n",
    "\n",
    "Y = Y*2-1  # set labels to -1, 1 as required by perceptron implementation\n",
    "\n",
    "m = dataset.shape[0] # it returns the dimension of our domain set \n",
    "print(m)\n",
    "permutation = np.random.permutation(m) # random permurtation of data mixing them inside the arrays. It returns an array of numbers from 0 to 259 (indices of data) to use as a mask to rearrange the aforementioned arrays\n",
    "\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]\n",
    "\n",
    "m_training = (7*m)//10 # choosing the training set to contain 70% of the actual data. Se il settanta percento non è un numero intero decido di usare la divisione intera, approsismando così per difetto la percentuale di dati nel training set\n",
    "\n",
    "#m_test needs to be the number of samples in the test set\n",
    "m_test = m-m_training\n",
    "\n",
    "#X_training = instances for training set\n",
    "# posso costruire un array di sette numeri da 0 a m-1 (corrispondenti alle righe). Una combinazione cioè di m numeri in m_training classi senza ripetizione. In questo modo potrò usare la mask random.sample(range(m),m_training) (che è già la lista con m_training elementi presi senza ripetizione dal range(m)) applicarla a X e trovare così il training set\n",
    "mask_training = random.sample(range(m),m_training)\n",
    "X_training = X[mask_training]\n",
    "Y_training = Y[mask_training]\n",
    "# devo controllare se in Y_training ci sono almeno dieci -1 e almeno dieci 1\n",
    "minus = 0\n",
    "plus = 0\n",
    "for num in Y_training :\n",
    "    if num == 1 :\n",
    "        plus += 1 \n",
    "    else :\n",
    "        minus +=1\n",
    "if plus >10 and minus > 10 : print(\"There are at least ten -1 and ten 1 in Y_training\")\n",
    "else : print(\"modify permutation or apply it multiple times\")\n",
    "\n",
    "# in questi posso mettere tutte le righe che non sono in x_training. Creo la maschera contenente tutti quegli indici che non si trovano in mask training tramite list comprehension \n",
    "mask_test = [num for num in permutation if num not in mask_training]\n",
    "X_test = X[mask_test]\n",
    "Y_test = Y[mask_test]\n",
    "# per verificare che gli elementi di X_training non siano in X_test confronto le maschere tramite inter=np.intersect1d(np.array(mask_training),np.array(mask_test)). Questo array contiene gli elementi ottenuti dall'operazione di intersezione logica tra i due insiemi/array mask_training e mask_test. Cioè restituisce un array con quegli elementi in comune tra i due array. Se è vuoto allora l'operazione di costruzione dei due set è andata a buon fine\n",
    "if np.intersect1d(np.array(mask_training),np.array(mask_test)).size == 0 : print(\"There are no conflicts between training and test set\")\n",
    "\n",
    "# print(Y_training) #to make sure that Y_training contains both 1 and -1\n",
    "print(m_test)\n",
    "\n",
    "print(\"Shape of training set: \" + str(X_training.shape))\n",
    "print(\"Shape of test set: \" + str(X_test.shape))\n",
    "print(\"Shape of training labels: \" + str(Y_training.shape))\n",
    "print(\"Shape of test labels: \" + str(Y_test.shape))\n",
    "\n",
    "#add a 1 to each sample (homogeneous coordinates)\n",
    "# Definisico nuovi array altrimenti i precedenti ogni volta che eseguo il codice vengono aggionrati e vi si aggiunge una colonna in più ad ogni run\n",
    "X_training_h = np.hstack((np.ones((m_training,1)),X_training))\n",
    "X_test_h = np.hstack((np.ones((m_test,1)),X_test))\n",
    "print(\"dimension of X in homogeneus coordinates is\", np.shape(X_training_h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1281acd0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAa8UlEQVR4nO3df5BU9Znv8feTEVk0ERYZKRzgwjUENyuuaF/FYnOTDTGoUZnl3rBSZsXElcquWZN4l1y4WqBGF7K4Jtequ0lhaYEll8huCBLNBl2yCVUpITuoCxgl4qIwI8K4BpKNXEV87h/njPT0dPfM6dPd50d/XlVT0+c5p7uf6Zl+5tvf7/ecr7k7IiKSLx9IOgEREak/FXcRkRxScRcRySEVdxGRHFJxFxHJoVOSTgBgzJgxPmnSpKTTEBHJlB07drzh7u3l9qWiuE+aNImurq6k0xARyRQze7XSPnXLiIjkkIq7iEgOqbiLiOSQiruISA6puIuI5FAqZsuIiExa/MSA2CsrPpNAJvmglruIJK5cYa8Wl8GpuIuI5JCKu4hIDqm4i4jkkIq7iEgOqbiLSOIqzYrRbJnaaSqkiKSCCnl9qeUuIpJDKu4iIjmk4i4ikkMq7iIiOaTiLiKSQyruIiI5pOIuIpJDgxZ3M3vIzA6b2e4y+/6HmbmZjQm3zczuN7O9ZrbTzC5sRNIiIlLdUFruq4HLS4NmNgH4NLC/KHwFMCX8Wgh8O36KIiIS1aDF3d23Am+W2fVN4GuAF8XmAA97YBswyszG1SVTEREZspouP2Bmc4Aed/9XMyve1QEcKNruDmMHyzzGQoLWPRMnTqwlDRGRzDp/2Y/49dsn3t8+Y3gbO+8c0ElSs8gDqmZ2GvC/gKVxntjdV7l7wd0L7e3tcR5KRCRTSgs7wK/fPsH5y35Ut+eopeV+DjAZ6Gu1jweeMbOLgR5gQtGx48OYiIiESgv7YPFaRG65u/sudz/L3Se5+ySCrpcL3f11YBNwfThrZgZw1N0HdMmIiEhjDWUq5DrgaWCqmXWb2Y1VDv8h8G/AXuAB4C/qkqWIiEQyaLeMu88fZP+kotsO3Bw/LRGR/DpjeFvZLpgzhrfV7Tl0hqqISJPtvPPyAYW83rNltBKTiEgC6lnIy1HLXUQkh1TcRURySMVdRCSHVNxFRHJIxV1EJIdU3EVEckjFXUQkh1TcRURySMVdRCSHVNxFRHJIxV1EJIdU3EVEckjFXUQkh1TcRURyaCgrMT1kZofNbHdR7OtmttPMnjOzJ83s7DBuZna/me0N91/YyORFRKS8obTcVwOlFx5e6e7nu/sFwOPA0jB+BTAl/FoIfLs+aYqISBSDFnd33wq8WRL7ddHm6YCHt+cAD3tgGzDKzMbVK1kRERmamldiMrN7gOuBo8AfheEO4EDRYd1h7GCZ+y8kaN0zceLEWtMQEZEyah5Qdffb3H0CsBb4Ug33X+XuBXcvtLe315qGiIiUUY81VNcCPwSWAT3AhKJ948OYiLSY6x54mp+9fLJHd+Y5o1l706UJZtRaamq5m9mUos05wIvh7U3A9eGsmRnAUXcf0CUjIvlWWtgBfvbym1z3wNMJZdR6Bm25m9k64BPAGDPrJmihX2lmU4H3gFeBL4aH/xC4EtgLvAV8vgE5i0jKlRb2weJSf4MWd3efXyb8YIVjHbg5blIiIhKPzlAVEcmhegyoijTEJfc8xaHfvPP+9tgPncr22y5LMKPWtvHZHlZu3sNrR45x9qgRLJo9lc7pHWWPnXnO6LJdMDPPGd3oNCWklrukUmlhBzj0m3e45J6nEsqotW18toclG3bRc+QYDvQcOcaSDbvY+Gz5yXBrb7p0QCHXbJnmUstdUqm0sA8Wl8ZauXkPx46f6Bc7dvwEKzfvqdh6VyFPloq7ZM7kxU8M2i0g9fXakWOR4mkUpVspD9QtI5kzlG4Bqa+zR42IFE+bqN1KeaDiLqk09kOnDnpMX7eANN6i2VMZMaytX2zEsDYWzZ6aUEbRVOtWyisVd0ml7bddNqQCn6VugSzrnN7B8rnT6Bg1AgM6Ro1g+dxpmenWyEO3UlTqc5fUKp72OHPFj+kp80bMSrdAHnRO78hMMS919qgRLff3o5a7ZELWuwUkWa3496OWu2RCX4uxlWY7SP204t+PBZeDSVahUPCurq6k0xARyRQz2+HuhXL71C0jIpJDKu4iIjmkPnfJjMvu+wkvHf7t+9tTzjqdp279RHIJiaSYWu6SCaWFHeClw7/lsvt+kkxCIik3lJWYHgKuAg67+3lhbCVwNfAO8DLweXc/Eu5bAtwInABucffNjUldWklpYR8sLsnTGqrJGkrLfTVweUnsKeA8dz8f+CWwBMDMPgpcC/x+eJ+/M7M2RKSlaA3V5A1a3N19K/BmSexJd3833NwGjA9vzwG+6+5vu/s+grVUL65jviKSAVpDNXn16HP/AvCP4e0O4EDRvu4wNoCZLTSzLjPr6u3trUMakmdTzjo9Ulyk1cUq7mZ2G/AusDbqfd19lbsX3L3Q3t4eJw1pAepzF4mm5qmQZnYDwUDrLD95mmsPMKHosPFhTEREmqimlruZXQ58DbjG3d8q2rUJuNbMhpvZZGAK8PP4aYqISBRDmQq5DvgEMMbMuoFlBLNjhgNPmRnANnf/ors/b2brgV8QdNfc7O4nyj+yiORVmxknyly3qi2oF9IEgxZ3d59fJvxglePvAe6Jk5SIpE+UNUjnXzKBR7btLxuvx+NDsJZu8b8PA/at+EzdHj/rdIaqZMIrFd60leJSX1HXIN3X+x+R4lEfv7SwQ7C27uTFT9Tl8fNAxV0y41t/ckG/Zd6+9ScXJJ1Sy4i6BmnUee5RH7/ShcorxVtxDVVdOEwyoa/l1fcG7Wt5Abn+aJ0WjV6DNOuPn0ZquUsmtGLLK00qrTVarzVIs/74aaTiLpnQii2vNIm6BunMc0ZHikd9/EpzbirFW3ENVRV3yYRWbHmlSef0DpbPndZvzGP53GkVu8TW3nTpgEJe7aqQUR9/34rPDCjk1WbLRH38PFCfu2TCotlT+/W5Q/5bXmnT+dhH6QT4HeD/AY8B049WPL7cVSHrqdq0x3I6p3fkupiXUstdMqEVW16pcsfISPFJFaYkVoq34lTFRlPLXTKj1VperaTagLl+57VRy11EEqcB8/pTcReRxGnAvP7ULSMiiVs0eypfefS5svGk3L5xF+u2H+CEO21mzL9kAnd3Tkssn6jUcheRwd1RYVZMhXjUawGVK+zV4o12+8ZdPLJt//tXtjzhziPb9nP7xl2J5FMLtdxFZGgqFfgKsnxRt3XbD1SMZ6X1rpa7iEiJcteirxZPIxV3EZESlRYVydJiI4MWdzN7yMwOm9nuothnzex5M3vPzAolxy8xs71mtsfMZjciaRFJv+seeJpJi594/+u6B55OOqUhq7SoSLXFRtJmKC331cDlJbHdwFxga3HQzD4KXAv8fnifvzOzNkSkpVz3wNNlLz9QqcCnbTGWuzun8bkZE99vqbeZ8bkZEzPT3w5DW2Zvq5lNKom9AGADP6LMAb7r7m8D+8xsL3AxkJ1/2SISW9TFOiB9A7B3d07LVDEvVe8+9w6geJi5O4wNYGYLzazLzLp6e3vrnIaISGtLbEDV3Ve5e8HdC+3t7UmlISKSS/We594DFI84jA9jIpF9eMkTvFsy86zNLLNnDLaSYR+A4++Vj0tz1Pul3gRca2bDzWwyMAX4eZ2fQ1pAucIOZPqMwVZSrrBXi0v9DWUq5DqCAdGpZtZtZjea2R+bWTdwKfCEmW0GcPfngfXAL4AfATe7+4lKjy1SSbnCXk6lMwlFWt1QZsvMr7Dr+xWOvwe4J05SIkOVpTMGRZpJPWCSaVk6Y1CkmVTcJZVOGWLNztIZgyLNpOIuqbR3+WfKFvgsnzHYStJ2xmkrMk9Bn2WhUPCurq6k0xARyRQz2+HuhXL71HIXEckhFXcRkRxScRcRySEVdxGRHFJxFxHJIS2QLZlRugDEzHNGs/amSxPMSKq5feMu1m0/oAu9JUQtd8mEqCv7SLJu37iLR7bt14XeEqTiLplQy8o+kpxKF3TThd6aR8VdROqu0gXddKG35lFxF5G6q3RBN13orXlU3CUTZp4zOlJcklXpgm71vNDbxmd7mLnix0xe/AQzV/yYjc9q0bdiKu6SCWtvunRAIddsmfS6u3Man5sxsWEXetv4bA9LNuyi58gxHOg5cowlG3apwBcZ9MJhZvYQcBVw2N3PC2OjgUeBScArwDx3/5WZGfC/gSuBt4Ab3P2ZwZLQhcNEJIqZK35Mz5FjA+Ido0bws8WfTCCjZMS9cNhq4PKS2GJgi7tPAbaE2wBXEKybOgVYCHy7loRFRKp5rUxhrxZvRYMWd3ffCpTON5sDrAlvrwE6i+IPe2AbMMrMxtUpVxERAM4eNSJSvBXV2uc+1t0PhrdfB8aGtzuA4oms3WFsADNbaGZdZtbV29tbYxoi0ooWzZ7KiGFt/WIjhrWxaPbUhDJKn9gDqh502keevOruq9y94O6F9vb2uGmISAvpnN7B8rnT6Bg1AiPoa18+dxqd08u2JVtSrdeWOWRm49z9YNjtcjiM9wDFc53GhzERkbrqnN6hYl5FrS33TcCC8PYC4LGi+PUWmAEcLeq+ERGRJhm05W5m64BPAGPMrBtYBqwA1pvZjcCrwLzw8B8STIPcSzAV8vMNyFlERAYxaHF39/kVds0qc6wDN8dNSkRE4tEZqiIiOaTFOiS1tNiDSO1U3CWV+hZ76NO32AOgAi8yBOqWkVTSYg8i8ai4SyppsQeReFTcJZW02INIPOpzl1Saf8mEfn3uxfHUuGNkmdjR5ufRJBuf7WHl5j28duQYZ48awaLZU3WGaIqp5S6p1OjFHmIrV9irxTNOi2Nkj1ruklp3d05LTzFvcSs37+HY8RP9YseOn2Dl5j1qvaeUWu4iMigtjpE9Ku4iMigtjpE9Ku4iMigtjpE9Ku4itag0Kyans2W0OEb2mKfgpJBCoeBdXV1JpyEikilmtsPdC+X2qeUuIpJDKu4iIjkUa567mX0ZuAkw4AF3/5aZjQYeBSYBrwDz3P1XMfMUabw7x4AfP7ltw2DZG8nlIxJDzS13MzuPoLBfDPwBcJWZfRhYDGxx9ynAlnBbJN1KCzsE23eOSSYfkZjidMv8HrDd3d9y93eBnwJzgTnAmvCYNUBnrAxFmqG0sA8WF0m5OMV9N/AxMzvTzE4jWBh7AjDW3Q+Gx7wOjC13ZzNbaGZdZtbV29sbIw0RESlVc3F39xeAbwBPAj8CngNOlBzjQNm5lu6+yt0L7l5ob2+vNQ0RESkj1oCquz8IPAhgZn8NdAOHzGycux80s3HA4fhpSku691z4j4Mntz84Dv7qxcY8lw0r3wVjwxrzfCINFmsqpJmdFX6fSNDf/n+BTcCC8JAFwGNxnkNaVGlhh2D73nMb83zL3hhYyDVbRjIs7iV/v2dmZwLHgZvd/YiZrQDWm9mNwKvAvLhJSgsqLeyDxetBhVxyJG63zMfKxP4dmBXncYfk8Vthx2rwE2BtcNENcNV9DX9aSbGd62HLXXC0G0aOh1lL4fwWalvoPSFFsrlYx+O3QteDJ7f9xMlt/TG3pp3r4Qe3wPHw+uJHDwTb0BoFXu8JKZHNyw/sWB0tLtnzwXHR4lvuOlnY+xw/FsRbgd4TUiKbxd1PRItL9vzViwMLebXZMke7o8XzRu8JKZHNbhlrK/9Ha20DY5JdUaY9jhwfdMWUi7cCvSekRDZb7hfdEC0u2bRzPXzzPLhjVPB95/rKx85aCsNKlnwbNiKID9W958IdI09+1Xva5eO3wp2jg8e+c3SwXU2Un1/vCSmRzeJ+1X1QuPFkq8Tagm0NHOVH3wDp0QOAnxwgrVTgzp8HV98PIycAFny/+v6hD6Y2el5934BnX+u6b8CzUoGP+vPrPSEltBKTpNM3z6vQzTIBvrq7/s93x8gq++qwdN6doyt3myx7c2C82T+/ZJJWYpLsydsAadQBz7z9/NJ0Ku6STpUGQrM6QFppYLNSPG8/vzSdirvUbs01/Qcg11xTv8eetRTaTu0fazu1+gBp1AHLYlHn1UcVdcCzHgPE0tJU3KU2a66BfT/tH9v30/oW+NLxoGrjQ1EHLEtFnVcfVdQBz7gDxNLyNKAqtWn0AGTUAcWoA5YiOaABVcmeqAOKOkNTpB8Vd0mnqAOKUQcsRXIuu8U9zuCZxDf549HiUVUaOKwU1xmaIv1ks7jHHTyT+BZsGljIJ388iNfDhpuixXWGpkg/sQZUzeyrwJ8RLIK9C/g8MA74LnAmsAP4U3d/p9rjRB5Q1eBZ/jV6wFYkBxoyoGpmHcAtQMHdzwPagGuBbwDfdPcPA78Cbqz1OSrS4JmISFVxu2VOAUaY2SnAacBB4JPAP4T71wCdMZ9jIA2eiYhUVXNxd/ce4F5gP0FRP0rQDXPE3d8ND+sGOsrd38wWmlmXmXX19vZGe3INnomIVBWnW+Z3gTnAZOBs4HTg8qHe391XuXvB3Qvt7e3RnlyDZ/lXqV9d/e0iQxJnJaZPAfvcvRfAzDYAM4FRZnZK2HofD/TET7OMq+5TMc+acoOk1Yp1swv5zvXBmqtHu4P59LOW6nR/yaw4fe77gRlmdpqZGTAL+AXwz8B/D49ZADwWL0XJhUqzX6rNimmmqItjiKRcnD737QQDp88QTIP8ALAK+J/ArWa2l2A65IN1yFOksbbcBceP9Y8dPxbERTIo1gLZ7r4MWFYS/jfg4jiPK9J0WhxDciabZ6iK1JsWx5CcidVyF2mo5RPh7aJB1eEjYcn+xjzXrKXlL22gxTEko9Ryl+YoVDhRuVK8tLBDsL18Yn3z6rP1b6PFRVJOxV2aY8fqaPHSwj5YPK43Kqy4VCkuknIq7tIcuh6QSFOpuEuTWMS4iMSh4i7Ncepp0eLDK5zcVCke15hzo8VFUk7FXZrjnd9Giy/ZP7CQN3K2zJe2DyzkY84N4iIZpKmQ0hzWVnmBlUoaVcgrUSGXHMlucX/81mCmhZ8ICsRFN+hCYmmmAVWRpspmt4zWUM2ekROixUUklmwW96hzpiV5s5bCsBH9Y8NG6AxQkQbJZnHXR/zsOX8eXH1/2FK34PvV9+t66SINks0+91oG5yR5589TMRdpkmy23LWGqohIVdlsuffNitFsGRGRsmou7mY2FXi0KPSfgaXAw2F8EvAKMM/df1V7ihVoDVURkYriLLO3x90vcPcLgIuAt4DvA4uBLe4+BdgSbouISBPVq899FvCyu78KzAHWhPE1QGednkNERIaoXsX9WmBdeHusux8Mb78OjC13BzNbaGZdZtbV29tbpzRERATqUNzN7FTgGuDvS/e5uwNe7n7uvsrdC+5eaG9vj5uGiIgUqUfL/QrgGXc/FG4fMrNxAOH3w3V4DhERiaAexX0+J7tkADYBC8LbC4DH6vAcIiISQazibmanA5cBG4rCK4DLzOwl4FPhtoiINFGsk5jc/bfAmSWxfyeYPSPS3871sOUuONoNI8cHFw3T5QhEGiKbZ6hK9uxcDz+4BY4fC7aPHgi2QQVepAGyeW0ZyZ4td50s7H2OHwviIlJ3Ku7SHEe7o8VFJBYVd2mOkeOjxUUkFhV3aQ6txCTSVCru0hxaiUmkqTRbRppHKzGJNI1a7iIiOaTiLiKSQyruIiI5pOIuIpJDKu4iIjlkwXoaCSdh1gu8mnQeoTHAG0knUUXa84P056j84lF+8dQzv//k7mVXO0pFcU8TM+ty90LSeVSS9vwg/Tkqv3iUXzzNyk/dMiIiOaTiLiKSQyruA61KOoFBpD0/SH+Oyi8e5RdPU/JTn7uISA6p5S4ikkMq7iIiOdRyxd3MHjKzw2a2uyT+l2b2opk9b2Z/UxRfYmZ7zWyPmc1OIj8ze9TMngu/XjGz51KW3wVmti3Mr8vMLg7jZmb3h/ntNLMLE8rvD8zsaTPbZWY/MLMzivY1+/WbYGb/bGa/CP/WvhzGR5vZU2b2Uvj9d8N4U1/DKvl9Ntx+z8wKJfdp2mtYJb+V4ft3p5l938xGpSy/r4e5PWdmT5rZ2WG8cb9fd2+pL+C/AhcCu4tifwT8EzA83D4r/P5R4F+B4cBk4GWgrdn5lez/W2BpmvIDngSuCG9fCfyk6PY/AgbMALYn9Pv9F+Dj4e0vAF9P8PUbB1wY3v4Q8Mswj78BFofxxcA3kngNq+T3e8BU4CdAoej4pr6GVfL7NHBKGP9G0euXlvzOKDrmFuA7jf79tlzL3d23Am+WhP8cWOHub4fHHA7jc4Dvuvvb7r4P2AtcnEB+QPBfHpgHrEtZfg70tYZHAq8V5fewB7YBo8xsXAL5fQTYGt5+CvhvRfk1+/U76O7PhLd/A7wAdIS5rAkPWwN0FuXYtNewUn7u/oK77ylzl6a+hlXye9Ld3w0P2wb0rd+Ylvx+XXTY6QTvmb78GvL7bbniXsFHgI+Z2XYz+6mZ/Zcw3gEcKDquO4wl5WPAIXd/KdxOS35fAVaa2QHgXmBJGE9Lfs8TvIkAPgtMCG8nmp+ZTQKmA9uBse5+MNz1OjA26RxL8qskjfl9gaA1DCnKz8zuCd8j1wF960s2LD8V98ApwGiCj0WLgPVhKzlt5nOy1Z4mfw581d0nAF8FHkw4n1JfAP7CzHYQfFR+J+F8MLMPAt8DvlLSqsODz+uJzlGull8aVMrPzG4D3gXWJpVbmMeA/Nz9tvA9shb4UqNzUHEPdAMbwo9GPwfeI7i4Tw8nW3kQfNTrSSA/zOwUYC7waFE4LfktADaEt/+ekx97U5Gfu7/o7p9294sI/jm+nGR+ZjaM4I2/1t37XrdDfR/Hw+99XYNNz7FCfpWkJj8zuwG4Crgu/AeZqvyKrOVk12DD8lNxD2wkGFTFzD4CnEpw1bZNwLVmNtzMJgNTgJ8nlOOngBfdvbsolpb8XgM+Ht7+JNDXbbQJuD6cETADOFrU9dA0ZnZW+P0DwO3Ad4rya+rrF34ifBB4wd3vK9q1ieCfJOH3x4riTXsNq+RXSVNfw0r5mdnlwNeAa9z9rRTmN6XosDnAi0X5Neb326hR47R+EbTcDgLHCVrsNxIU80eA3cAzwCeLjr+NoKW3h3BGSLPzC+OrgS+WOT7x/IA/BHYQzErYDlwUHmvA/wnz20XRLIsm5/dlglkLvwRWEJ6ZndDr94cEXS47gefCryuBM4EtBP8Y/wkYncRrWCW/Pw5fz7eBQ8DmJF7DKvntJei77ot9J2X5fS+sLzuBHxAMsjb096vLD4iI5JC6ZUREckjFXUQkh1TcRURySMVdRCSHVNxFRHJIxV1EJIdU3EVEcuj/A1WA+DJ7c1rIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rappresentiamo i punti per vedere se sono linearly separable o no \n",
    "x = X[:,0:2] \n",
    "# però definisco i punti che hanno -1 come label red e quelli che hanno uno blue\n",
    "y = Y\n",
    "x_blue1 = x[np.where(y == 1),0] \n",
    "x_blue2 = x[np.where(y == 1),1]\n",
    "# questa qua dentro l'argomento di x è una mask che mi dice gli indici dove in y ci sono uno oppure meno uno\n",
    "x_red1 = x[np.where(y == -1),0]\n",
    "x_red2 = x[np.where(y == -1),1]\n",
    "# devo rappresentare le due x \n",
    "plt.scatter(x_blue1,x_blue2)\n",
    "plt.scatter(x_red1,x_red2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere un'idea sulla convergenza dell'algoritmo rappresento due delle features dei punti del dominio, coloro diversamente quei punti la cui label è diversa.\n",
    "I dati non sono linearmente separabili quindi il perceptron non converge nemmeno dopo tantissime iterazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron1(x,y,max_n_iter) :\n",
    "    curr_w = np.zeros((len(x[0]),1)) \n",
    "    best_w = np.zeros((len(x[0]),1))\n",
    "    num_samples = len(y)\n",
    "    best_error = 1\n",
    "    num_iter = 1\n",
    "    Z = x*y.reshape((-1,1))\n",
    "    a = (Z @ curr_w <= 0).reshape((-1,)) \n",
    "    num_misclassified = np.count_nonzero(a)\n",
    "    index_misclassified = np.random.choice(np.arange(num_samples)[a])\n",
    "\n",
    "    while((index_misclassified != -1) and (num_iter < max_n_iter)) : \n",
    "        curr_w += Z[index_misclassified].reshape((-1,1)) \n",
    "        num_iter += 1\n",
    "        a = (Z @ curr_w <= 0).reshape((-1,))\n",
    "        num_misclassified = np.count_nonzero(a)\n",
    "        e = num_misclassified/num_samples\n",
    "        if e < best_error :\n",
    "            best_error = e\n",
    "            best_w = curr_w\n",
    "        if num_misclassified > 0 :\n",
    "            index_misclassified = np.random.choice(np.arange(num_samples)[a]) \n",
    "        else :\n",
    "            index_misclassified = -1\n",
    "\n",
    "    return best_w.reshape((-1,)),best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuovo perceptron perchè il precedente restituisce un errore del cinquanta percento. Poichè agisce ogni iterazione su tutti i dati è molto probabile che il lavoro che viene fatto per migliorare l'inclinazione della retta quando si trova un punto misclassified venga annullato dal successivo upgrade di w. Infatti visto che vengono comunque ciclati tutti i punti che sono misclassified il peso w ha la stessa probabilità di \"migliorare\" e di \"peggiorare\" in ciascuna iterazione (del ciclo while).\n",
    "def perceptron2(X, Y, max_num_iterations):\n",
    "    curr_w = np.zeros(shape = (max_num_iterations, np.shape(X)[1]))\n",
    "    new_w = np.zeros(shape = (max_num_iterations+1, np.shape(X)[1])) \n",
    "    num_samples = np.shape(X)[0]  \n",
    "    number_misclassified = [] \n",
    "    num_iter = 0 \n",
    "    index_misclassified = 0\n",
    "    while ((index_misclassified != -1) and (num_iter < max_num_iterations)):\n",
    "        index_misclassified = -1\n",
    "        curr_w[num_iter,:] = new_w[num_iter,:]\n",
    "        indexes_list = [i for i in range(num_samples-1) if Y[i]*np.dot(curr_w[num_iter,:],X[i,:]) <= 0] \n",
    "        number_misclassified.append(len(indexes_list))\n",
    "        index_misclass_random = random.choice(indexes_list) \n",
    "        new_w[num_iter+1,:] = curr_w[num_iter,:] + Y[index_misclass_random]*X[index_misclass_random,:]\n",
    "        if len(indexes_list) != 0 : index_misclassified = 0\n",
    "        num_iter += 1\n",
    "    new_w = np.delete(new_w, (-1), axis = 0) \n",
    "    # perchè devo eliminare l'ultima riga di new_w ottenuta durante l'ultima iterazione che ha assegnato l'ultima posizione + 1 al vettore dei pesi \n",
    "    best_w = new_w[np.argmin(number_misclassified),:]\n",
    "    best_error = min(number_misclassified)/num_samples\n",
    "    return best_w, best_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Perceptron mentre lo stavo sviluppando \n",
    "\n",
    "```python\n",
    "import random \n",
    "#print(X_training_h)\n",
    "#print(Y_training)\n",
    "max_num_iterations = 100\n",
    "curr_w = np.zeros(shape = (max_num_iterations, np.shape(X_training_h)[1]))\n",
    "new_w = np.zeros(shape = (max_num_iterations+1, np.shape(X_training_h)[1]))   \n",
    "num_iter = 0\n",
    "index_misclassified = 0\n",
    "numeroerrori = []\n",
    "while ((index_misclassified != -1) and (num_iter < max_num_iterations)) : \n",
    "    index_misclassified = -1\n",
    "    curr_w[num_iter,:] = new_w[num_iter,:]\n",
    "    listaindici = [i for i in range(np.shape(X_training_h)[0]-1) if (Y_training[i]*np.dot(curr_w[num_iter,:],X_training_h[i,:]) <= 0)] \n",
    "    numeroerrori.append(len(listaindici))\n",
    "    # così ho trovato una lista contenente tutti i samples misclassified noto il vettore dei pesi per quell'iterazione naturalmente. La lunghezza di tale lista sono i samples misclassified e scegliendo uno degli indici a caso posso upgradare il vettore dei pesi con uno nuovo \n",
    "    indice_misclass_random = random.choice(listaindici) \n",
    "    # Scelto l'indice a caso utilizzo la riga di X e l'elemento di Y corrispondenti a tale indice per aggiornare il vettore dei pesi\n",
    "    new_w[num_iter+1,:] = curr_w[num_iter,:] + Y_training[indice_misclass_random]*X_training_h[indice_misclass_random,:] # Aggiorno il peso \n",
    "    if len(listaindici) != 0 : index_misclassified = 0 # controllo se l'algoritmo è arrivato a convergere\n",
    "    num_iter += 1\n",
    "new_w = np.delete(new_w, (-1), axis = 0)\n",
    "# a new_w devo eliminare l'ultima riga perchè è quella che corrisponde alla centesima iterazione che non ha ricevuto calcolo dell'errore in quanto il comando che appende l'errore si trova prima di quello che aggiorna il vettore dei pesi e arrivati a 100 new_w arriva alla riga 101 mentre l'errore non viene aggiornato più in quanto il while è interrotto\n",
    "\n",
    "erroreminimo = min(numeroerrori)/np.shape(X_training_h)[0]\n",
    "pesimigliori = new_w[np.argmin(numeroerrori),:]\n",
    "print(\"l'errore minimo che è stato raggiunto durante l'algoritmo è: \", erroreminimo)\n",
    "print(\"il miglior vettore dei pesi ottenuto è\", pesimigliori)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso devo utilizzare il vettore dei pesi w fornito dall'algoritmo per predire le labels dei punti appartenenti al test set: <br>\n",
    "$y_i = \\text{sign} (\\langle \\mathbf{w},\\mathbf{x_i} \\rangle + b)$ <br>\n",
    "I pesi $ \\mathbf{w}$ sono quelli ottenuti dal perceptron sul training set, predicendo quindi le labels del test set abbiamo già selezionato la best hypotesis e stiamo simulando l'algoritmo su dei dati \"reali\". L'errore che troviamo alla fine è una stima del true error in confronto al training error ottenuto dall'algoritmo sul set di allenamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "[  -5. -255.  555. -125.] 0.06593406593406594\n",
      "Test Error of perceptron (100 iterations): 0.10256410256410256\n"
     ]
    }
   ],
   "source": [
    "w_found, error = perceptron2(X_training_h, Y_training, 100)\n",
    "# ho constatato che comunque il vettore dei pesi trovato è sempre diverso al variare delle iterazioni\n",
    "# print(\"Training error of perceptron is: \", error)\n",
    "# I dati nel test set sono X_test_h in coordinate omogenee, di cui sono noti anche le labels pertanto possiamo fare un loop in cui confrontiamo per ogni label predetta quella teorica ed eventualmente incrementiamo un iterable che chiamiamo num_error\n",
    "# num_error = 0\n",
    "num_test_samples = np.shape(X_test_h)[0] # sono 78 in proporzione 30 percento su 260\n",
    "num_training_samples = np.shape(X_training_h)[0]\n",
    "print(num_test_samples)\n",
    "print(w_found, error)\n",
    "\n",
    "#num_error = 0 \n",
    "Y_predict = [np.sign(np.dot(w_found,X_test_h[i,:])) for i in range(num_test_samples)]\n",
    "num_error = np.count_nonzero(Y_test - Y_predict)\n",
    "#for i in range(num_test_samples) :\n",
    "    #y_predict = np.sign(np.dot(w_found,X_test_h[i,:]))\n",
    "    #if y_predict != Y_test[i] :\n",
    "        #num_error += 1\n",
    "true_loss_estimate = num_error/num_test_samples\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provo ad usare la funzione che cito all'inzio del notebook per separare il dominio in training e test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui è presente anche il modo in cui all'inizio del notebook abbiamo suddiviso il dominio in training set e test set. Volevo vedere se ci fossero differenze in termini di errori dell'algoritmo ma ho trovato gli stessi ordini di grandezza. <br>\n",
    "```python\n",
    "permutation = np.random.permutation(m) \n",
    "X = X[permutation] \n",
    "Y = Y[permutation] \n",
    "m_training = (7*m)//10 \n",
    "m_test = m-m_training \n",
    "mask_training = random.sample(range(m),m_training) \n",
    "X_train = X[mask_training] \n",
    "Y_train = Y[mask_training] \n",
    "mask_test = [num for num in permutation if num not in mask_training] \n",
    "X_test = X[mask_test] \n",
    "Y_test = Y[mask_test] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on training set is:  0.07692307692307693\n",
      "Error rate on test set is:  0.02564102564102564\n"
     ]
    }
   ],
   "source": [
    "# Dalla stessa suddivisione in training set e test set utilizzo la logisitc regression per predire la binary classification dei giocatori di basket \n",
    "logreg = linear_model.LogisticRegression(C=1e5) # una grossa costante c implica il non considerare quasi la regolarizzazione\n",
    "logreg.fit(X_train, Y_train) # in questo modo alleno l'algoritmo sui dati nel training set\n",
    "Y_predict_training = logreg.predict(X_train)\n",
    "misclass_training = 0 # inizializzo il numero di misclassified labels per incrementarlo ogni volta che la Y_predict è diversa da Y_training per ciascuna riga \n",
    "for i in range(num_training_samples-1) :\n",
    "    if Y_predict_training[i] != Y_train[i] :\n",
    "        misclass_training += 1\n",
    "print(\"Error rate on training set is: \", misclass_training/num_training_samples) \n",
    "\n",
    "Y_predict_test = logreg.predict(X_test)\n",
    "misclass_test = 0\n",
    "# print(Y_predict_test,\"\\n\", Y_test)\n",
    "for i in range(num_test_samples-1) :\n",
    "    if Y_predict_test[i] != Y_test[i] :\n",
    "        misclass_test += 1\n",
    "print(\"Error rate on test set is: \", misclass_test/num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso posso restringere i dati a sole due features. Tra le tre a disposizione scelgo di escludere l'età in quanto non provvede una comprensione migliore del problema. Intuitivamnete infatti l'età dei giocatori non è correlata al loro ruolo e nemmeno dunque alle altre due features, non è dipendente dalle altre due bensì direttamente ininfluente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make the plot we need to reduce the data to 2D, so we choose two features\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_list = ['height', 'weight', 'age']\n",
    "labels_list = ['Center', 'Point guard']\n",
    "\n",
    "# select a pair of features\n",
    "index_feature1 = 0 # we choose height\n",
    "index_feature2 = 1 # and weight (age of course is not very meaningful)\n",
    "features = [index_feature1, index_feature2]\n",
    "\n",
    "feature_name0 = features_list[features[0]]\n",
    "feature_name1 = features_list[features[1]]\n",
    "\n",
    "X_reduced = X[:,features]\n",
    "\n",
    "# suddivido nuovamene in training set e test set \n",
    "\n",
    "X_train2, X_test2, Y_train2 ,Y_test2 = train_test_split(X_reduced,Y,test_size=0.3,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on training set is:  0.07692307692307693\n",
      "Error rate on test set is:  0.02564102564102564\n"
     ]
    }
   ],
   "source": [
    "# Adesso alleniamo l'algoritmo su questi dati \n",
    "logreg2 = linear_model.LogisticRegression(C=1e5) \n",
    "logreg2.fit(X_train2, Y_train2) \n",
    "Y_predict_training2 = logreg2.predict(X_train2)\n",
    "misclass_training2 = 0  \n",
    "for i in range(num_training_samples-1) :\n",
    "    if Y_predict_training2[i] != Y_train2[i] :\n",
    "        misclass_training2 += 1\n",
    "print(\"Error rate on training set is: \", misclass_training2/num_training_samples) \n",
    "\n",
    "Y_predict_test2 = logreg2.predict(X_test2)\n",
    "misclass_test2 = 0\n",
    "\n",
    "for i in range(num_test_samples-1) :\n",
    "    if Y_predict_test2[i] != Y_test2[i] :\n",
    "        misclass_test2 += 1\n",
    "print(\"Error rate on test set is: \", misclass_test2/num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NON CAPISCO PERCHE' MA E' TROPPO PESANTE DA ESEGUIRE E PROVOCA ANCHE UN GROSSO RALLENTAMENTO NEL SALVATAGGIO DEL NOTEBOOK QUINDI NON ESEGUIRE!!!\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "# proviamo a rappresentare i dati e i predict sulle due features che abbiamo scelto \n",
    "h = .1  # step size in the mesh HO DOVUTO CAMBIARLO DI UN ORDINE DI GRANDEZZA ALTRIMENTI VENIVA RESTITUITO UN ERRORE DI MEMORIA \n",
    "x_min, x_max = X_reduced[:, 0].min() - .5, X_reduced[:, 0].max() + .5\n",
    "y_min, y_max = X_reduced[:, 1].min() - .5, X_reduced[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = logreg2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train2[:, 0], X_train2[:, 1], c=Y_train2, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel(feature_name0)\n",
    "plt.ylabel(feature_name1)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Training set')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the test points \n",
    "plt.scatter(X_test2[:, 0], X_test2[:, 1], c=Y_test2, edgecolors='k', cmap=plt.cm.Paired, marker='s')\n",
    "plt.xlabel(feature_name0)\n",
    "plt.ylabel(feature_name1)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Test set')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression #\n",
    "I dati presentano ben 500 osservazioni di cui 13 input per la regressione lineare e un output, la predizione, cioè il prezzo di una casa. Bisogna dunque costruire una linear regression in grado di correlare tra loro i tredici input e restituire il prezzo della casa noti dei dati già etichettati nel training set per allenare l'algoritmo.\n",
    "\n",
    "The variable names are as follows:\n",
    "\n",
    "CRIM: per capita crime rate by town.\n",
    "\n",
    "ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS: proportion of nonretail business acres per town.\n",
    "\n",
    "CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "NOX: nitric oxides concentration (parts per 10 million).\n",
    "\n",
    "RM: average number of rooms per dwelling.\n",
    "\n",
    "AGE: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "DIS: weighted distances to five Boston employment centers.\n",
    "\n",
    "RAD: index of accessibility to radial highways.\n",
    "\n",
    "TAX: full-value property-tax rate per $10,000.\n",
    "\n",
    "PTRATIO: pupil-teacher ratio by town.\n",
    "\n",
    "B: 1000*(Bk – 0.63)2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "LSTAT: % lower status of the population.\n",
    "\n",
    "MEDV: Median value of owner-occupied homes in $1000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  \\\n0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n495  0.17899   0.0   9.69     0  0.585  5.670  28.8  2.7986    6  391   \n496  0.28960   0.0   9.69     0  0.585  5.390  72.9  2.7986    6  391   \n497  0.26838   0.0   9.69     0  0.585  5.794  70.6  2.8927    6  391   \n498  0.23912   0.0   9.69     0  0.585  6.019  65.3  2.4091    6  391   \n499  0.17783   0.0   9.69     0  0.585  5.569  73.5  2.3999    6  391   \n\n     PTRATIO       B  LSTAT  MEDV  \n0       15.3  396.90   4.98   240  \n1       17.8  396.90   9.14   216  \n2       17.8  392.83   4.03   347  \n3       18.7  394.63   2.94   334  \n4       18.7  396.90   5.33   362  \n..       ...     ...    ...   ...  \n495     19.2  393.29  17.60   231  \n496     19.2  396.90  21.14   197  \n497     19.2  396.90  14.10   183  \n498     19.2  396.90  12.92   212  \n499     19.2  395.77  15.10   175  \n\n[500 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>240</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>216</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>347</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>334</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>362</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.17899</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>5.670</td>\n      <td>28.8</td>\n      <td>2.7986</td>\n      <td>6</td>\n      <td>391</td>\n      <td>19.2</td>\n      <td>393.29</td>\n      <td>17.60</td>\n      <td>231</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>0.28960</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>5.390</td>\n      <td>72.9</td>\n      <td>2.7986</td>\n      <td>6</td>\n      <td>391</td>\n      <td>19.2</td>\n      <td>396.90</td>\n      <td>21.14</td>\n      <td>197</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>0.26838</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>5.794</td>\n      <td>70.6</td>\n      <td>2.8927</td>\n      <td>6</td>\n      <td>391</td>\n      <td>19.2</td>\n      <td>396.90</td>\n      <td>14.10</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>0.23912</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>6.019</td>\n      <td>65.3</td>\n      <td>2.4091</td>\n      <td>6</td>\n      <td>391</td>\n      <td>19.2</td>\n      <td>396.90</td>\n      <td>12.92</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.17783</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>5.569</td>\n      <td>73.5</td>\n      <td>2.3999</td>\n      <td>6</td>\n      <td>391</td>\n      <td>19.2</td>\n      <td>395.77</td>\n      <td>15.10</td>\n      <td>175</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 14 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "filename = \"data/house.csv\"\n",
    "dataDescription = pd.read_csv(filename,delimiter = \";\")\n",
    "display(dataDescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adesso voglio suddividere in training, validation e test set con proporzione 60,20,20 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}